{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4154a0d-6685-46f4-acd6-c03ffec8474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import random\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "from dataset import get_dataloaders, num_class\n",
    "from torch.autograd import Variable\n",
    "from resnet import ResNet\n",
    "from datetime import datetime\n",
    "#from torchsummary import summary\n",
    "#import wandb \n",
    "#print('wandb version',wandb.__version__)\n",
    "torch.backends.cudnn.enabled = False\n",
    " \n",
    "'''\n",
    "./c-submit --require-gpu-mem=\"8G\" --gpu-count=\"1\" --require-mem=\"40G\" --require-cpus=\"8\" --node=\"dlc-lugia\" --priority=\"low\" khrystynafaryna 9548 72 doduo1.umcn.nl/khrystynafaryna/kf_container_invariant:latest python3 /data/pathology/projects/autoaugmentation/from_chansey_review/invariant/train.py --val_set  val_cwh_umcu_ --m 0 --n 0 --dataset camelyon17 --dataroot '/data/pathology/projects/autoaugmentation/from_chansey_upd/data/lymph/patches/'\n",
    "\n",
    "./c-submit --require-gpu-mem=\"8G\" --gpu-count=\"1\" --require-cpus=\"4\"  --require-mem=\"20G\" --node=\"dlc-sycorax\"--interactive --priority=\"high\" --env CODEBASE=/data/pathology/projects/autoaugmentation/from_chansey_review/invariant  khrystynafaryna 9548 4  doduo1.umcn.nl/khrystynafaryna/kf_container_invariant:latest\n",
    "\n",
    "\n",
    "'''\n",
    "parser = argparse.ArgumentParser(\"camelyon17\")\n",
    "parser.add_argument('--dataroot', type=str, default='/data/pathology/projects/autoaugmentation/from_chansey_upd/data/tiger/patches/', help='location of the data corpus')\n",
    "parser.add_argument('--dataset', type=str, default='camelyon17',choices=['camelyon17','none','tiger'],\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--train_set', type=str, default='training_', help='train file name')\n",
    "parser.add_argument('--val_set', type=str, default='val_rh_umcu_', help='val file name')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.003, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.00003, help='min learning rate')\n",
    "parser.add_argument('--report_freq', type=float, default=1, help='report frequency')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='num of training epochs')\n",
    "parser.add_argument('--model', type=str, default='resnet18', help='path to save the model')\n",
    "parser.add_argument('--save', type=str, default='/data/pathology/projects/autoaugmentation/from_chansey_review/invariant/experiments/', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--sample_weighted_loss', type=bool, default=True, help=\"sample weights in loss function\")\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=2e-4, help='weight decay')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "parser.add_argument('--use_cuda', type=bool, default=True, help=\"use cuda default True\")\n",
    "parser.add_argument('--use_parallel', type=bool, default=False, help=\"use data parallel default False\")\n",
    "parser.add_argument('--num_workers', type=int, default=8, help=\"num_workers\")\n",
    "parser.add_argument('--randaugment', type=bool, default=False, help='use randaugment augmentation')\n",
    "parser.add_argument('--m', type=int, default=0, help=\"magnitude of randaugment\")\n",
    "parser.add_argument('--n', type=int, default=0, help=\"number of layers randaugment\")\n",
    "parser.add_argument('--randomize', type=bool, default=True, help=\"randomize magnitude in randaugment\")\n",
    "parser.add_argument('--randaugment_transforms_set', type=str, default='invariant',choices=['review','midl2021','original','midl2021_tr2eb','midl2021_trsh2eb','manual','invariant'],help='which set of randaugment transforms to use')\n",
    "parser.add_argument('--lr_schedule', type=str, default='rlop', choices = ['cos','exp','rlop'], help = \"which lr scheduler to use\")\n",
    "parser.add_argument('--optimizer_type', type=str, default='adam', choices = ['sdg','adam','rms'], help = \"which optimizer to use\")\n",
    "parser.add_argument('--save_best', type=bool, default=True, help=\"If True, updating model weights only whe minimum of loss occurs, else updating weights every epoch\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "arg_dict={}\n",
    "\n",
    "arg_dict['val_set']=args.val_set\n",
    "arg_dict['batch_size']=args.batch_size\n",
    "arg_dict['learning_rate']=args.learning_rate\n",
    "arg_dict['learning_rate_min']=args.learning_rate_min\n",
    "arg_dict['randaugment']=args.randaugment\n",
    "arg_dict['randomize']=args.randomize\n",
    "arg_dict['m']=args.m\n",
    "arg_dict['n']=args.n\n",
    "arg_dict['randaugment_transforms_set']=args.randaugment_transforms_set\n",
    "arg_dict['optimizer_type']=args.optimizer_type\n",
    "arg_dict['lr_schedule']=args.lr_schedule\n",
    "arg_dict['dataset']=args.dataset\n",
    "\n",
    "project_name='invariant'+args.val_set[3:-1]\n",
    "\n",
    "time_stamp= args.dataset+'_'+args.val_set+'n_'+str(args.n)+'_m_'+str(args.m)+'_t_'+args.randaugment_transforms_set+'/'+str(datetime.now())\n",
    "#wandb.init(project=project_name, name = args.val_set+'n_'+str(args.n)+'_m_'+str(args.m)+'_t_'+args.randaugment_transforms_set,notes=\"first logging with wandb\", \n",
    "#  config=arg_dict,dir='/data/pathology/projects/autoaugmentation/from_chansey_review')\n",
    "args.save = os.path.join(args.save,time_stamp)\n",
    "os.makedirs(args.save, exist_ok=True)\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "sub_policies=None\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "num_classes=2\n",
    "\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "logging.info('no gpu device available')\n",
    "sys.exit(1)\n",
    "\n",
    "# np.random.seed(args.seed)\n",
    "# torch.cuda.set_device(args.gpu)\n",
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "cudnn.enabled=True\n",
    "\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "model = ResNet(dataset='imagenet-dropout', depth=18, num_classes=num_classes, bottleneck=True)\n",
    "print(model)\n",
    "model = model.cuda()\n",
    "\n",
    "#summary(model,(3,128,128,15))\n",
    "\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "if args.optimizer_type=='sdg':\n",
    "optimizer = torch.optim.SGD(model.parameters(),args.learning_rate,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "elif args.optimizer_type=='adam':\n",
    "optimizer = torch.optim.Adam(model.parameters(),args.learning_rate)\n",
    "elif args.optimizer_type=='rms':\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),args.learning_rate,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "else:\n",
    "print('This optimizer is not implemented')\n",
    "\n",
    "\n",
    "train_queue, valid_queue = get_dataloaders(args.dataset, args.batch_size, args.num_workers, dataroot=args.dataroot, train_set=args.train_set, val_set=args.val_set, augmenter='morphology')\n",
    "\n",
    "\n",
    "if args.lr_schedule=='rlop':\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, min_lr=1e-5, eps=1e-08, verbose=True)\n",
    "elif args.lr_schedule=='cos':\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.epochs))\n",
    "else:\n",
    "print('This lr schedule is not implemented')\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "#scheduler.step()\n",
    "logging.info('Epoch: %d ', epoch)\n",
    "\n",
    "train_acc, train_obj = train(train_queue, model, criterion, optimizer)\n",
    "logging.info('train_acc %f', train_acc)\n",
    "\n",
    "valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
    "logging.info('valid_acc %f', valid_acc)\n",
    "scheduler.step(valid_obj)\n",
    "\n",
    "\n",
    "if args.save_best:\n",
    "\n",
    "  if epoch == 0:\n",
    "    best_valid_obj=valid_obj.copy()\n",
    "    utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "  else:\n",
    "\n",
    "      if valid_obj < best_valid_obj:\n",
    "        best_valid_obj=valid_obj.copy()\n",
    "        utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "        logging.info('New best valid_obj achieved %f, overwriting the model  weights...', valid_obj)\n",
    "      else:\n",
    "        logging.info('Not updating weights this epoch, valid_obj %f, higher than minimum...', valid_obj)\n",
    "\n",
    "else:\n",
    "  utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "  logging.info('Option save_best is off, updating weights every epoch...')\n",
    "\n",
    "\n",
    "utils.history_to_file(io_path = args.save+\"/log.csv\", epoch = epoch, lr = optimizer.param_groups[0]['lr'], train_obj = train_obj, train_metric = train_acc, valid_obj = valid_obj, valid_metric = valid_acc)\n",
    "utils.plot_history(history_path = args.save+\"/log.csv\", plot_path = args.save+\"/log.png\")\n",
    "\n",
    "       \n",
    "def train(train_queue, model, criterion, optimizer, sample_weighted_loss=True):\n",
    "\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    model.train()\n",
    "    for step, (inpt, target) in enumerate(train_queue):\n",
    "        #print(\"input shape\",inpt.shape)\n",
    "        #print(\"label shape\",target.shape)\n",
    "      \n",
    "        \n",
    "      \n",
    "        inpt = Variable(inpt.type(torch.FloatTensor), requires_grad=False).cuda()\n",
    "\n",
    "        target = Variable(target, requires_grad=False).cuda(non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inpt)\n",
    "        if sample_weighted_loss:\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            batch_weight = Variable(torch.Tensor(compute_label_weights(target.detach().cpu().numpy())), requires_grad=False).cuda()\n",
    "            loss = loss*batch_weight\n",
    "            loss = torch.mean(loss)\n",
    "\n",
    "        else:\n",
    "            loss = criterion(logits, target)\n",
    "        #print(loss.detach().cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "        prec1 = utils.accuracy(logits.detach(), target.detach())\n",
    "        n = inpt.size(0)\n",
    "        objs.update(loss.detach().cpu().numpy(), n)\n",
    "        top1.update(prec1.detach().cpu().numpy(), n)\n",
    "       \n",
    "        #if step % args.report_freq == 0:\n",
    "        #    logging.info('train %03d %e %f', step, objs.avg, top1.avg)\n",
    "        #    #logging.info('train auc: %f',roc_auc_score(target.detach().cpu().numpy(),logits.detach().cpu().numpy()[:,1]) \n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "def infer(valid_queue, model, criterion):\n",
    "\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (inpt, target) in enumerate(valid_queue):\n",
    "\n",
    "            inpt = Variable(inpt.type(torch.FloatTensor)).cuda()\n",
    "            target = Variable(target).cuda(non_blocking=True)\n",
    "\n",
    "\n",
    "            logits = model(inpt)\n",
    "            loss = criterion(logits, target)\n",
    "            batch_weight = Variable(torch.Tensor(compute_label_weights(target.detach().cpu().numpy())), requires_grad=False).cuda()\n",
    "            loss = loss*batch_weight\n",
    "            loss = torch.mean(loss)\n",
    "\n",
    "            prec1 = utils.accuracy(logits, target)\n",
    "            n = inpt.size(0)\n",
    "\n",
    "            objs.update(loss.detach().cpu().numpy(), n)\n",
    "            top1.update(prec1.detach().cpu().numpy(), n)\n",
    "\n",
    "\n",
    "            #if step % args.report_freq == 0:\n",
    "            #    logging.info('valid %03d %e %f', step, objs.avg, top1.avg)\n",
    "                \n",
    "    return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "\n",
    "def compute_label_weights(y_true, one_hot=False):\n",
    "\n",
    "    if one_hot:\n",
    "        y_true_single = np.argmax(y_true, axis=-1)\n",
    "    else:\n",
    "        y_true_single = y_true\n",
    "\n",
    "    w = np.ones(y_true_single.shape[0])\n",
    "    for idx, i in enumerate(np.bincount(y_true_single)):\n",
    "        w[y_true_single == idx] *= 1/(i / float(y_true_single.shape[0]))\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
